## 04. Cache

#### 功能

自己实现一个在内存里的缓存（cache），使用的策略（policy）是经典的Least-Recenlty-Used（最近最少使用）。之前在LeetCode上做过一个LRU的题，使用的是一个双向链表和哈希表。但这里的实现会更加复杂、更加底层。缓存的作用，一句话来概括，就是利用数据访问的时间局部性，通过牺牲一部分空间，从而实现更加快速的访问。典型的“时间-空间权衡（trade-off）”。

#### 实现

首先有个抽象的`cache`基类，只定义接口，并不指定具体的策略。

```c++
class LEVELDB_EXPORT Cache;

// Create a new cache with a fixed size capacity
LEVELDB_EXPORT Cache *NewLRUCache(size_t capacity);

class LEVELDB_EXPORT Cache {
public:
    Cache() = default;

    Cache(const Cache&) = delete;
    Cache& operator=(const Cache&) = delete;

    // Destroys all existing entries by calling the "deleter"
    // function that was passed to the constructor.
    virtual ~Cache();

    // Opaque handle to an entry stored in the cache
    struct Handle {};

    virtual Handle* Insert(const Slice& key, void* value, size_t charge,
                        void (*deleter)(const Slice& key, void *value)) = 0;
    virtual Handle* Lookup(const Slice& key) = 0;
    
    // Release a mapping returned by a previous Lookup()
    virtual void Release(Handle* handle) = 0;

    virtual void* Value(Handle* handle) = 0;

    virtual void Erase(const Slice& key) = 0;

    // Returns a new numeric id.
    virtual uint64_t NewId() = 0;
    
    // Remove all cache entries that are not actively in use.
    virtual void Prune() {}

    // Return an estimate of the combined charges of all elements stored
    // in the cache
    virtual size_t TotalCharge() const = 0;
    
private:
    void LRU_Remove(Handle* e);
    void LRU_Append(Handle* e);
    void Unref(Handle* e);

    struct Rep;
    Rep* rep_;
};
```

实际只实现了`LRU`这么一种策略算法。`cache`中的`handle`只是一个占位，具体的实现如下：

```c++
// LRU caceh implementation
struct LRUHandle {
    void *value;
    void (*deleter)(const Slice&, void *value);
  	// hash冲突时指向下一个LRUHandle*
    LRUHandle* next_hash;
  	// 双向链表
    LRUHandle* next;
    LRUHandle* prev;
    size_t charge;
    size_t key_length;
    bool in_cache;
    uint32_t refs;      // references
    uint32_t hash;      // hash of key
    char key_data[1];   // beginning of key

    Slice key() const {
        assert(next != this);

        return Slice(key_data, key_length);
    }
};
```

LRUCache主要由两部分组成：

- Hash table，用于快速检索数据
- LRU：用来维护数据项的新旧信息

![image-20201019201708446](https://img.plantree.me/image-20201019201708446.png)

Hash表也是自己实现的，说是比内置的hash table速度更快。实现的方式基于Yujie Liu等人的论文《Dynamic-Sized Nonblocking Hash Table》，Hash表实现了对于插入、删除和查找的时间复杂度为`O(1)`。但是当hash表的数据量增大的时候，改变bucket的个数，需要对所有的数据重新散列。这篇文章中实现的hash table可以在resize的过程中**不阻塞其他并发的读写请求**。

##### Dynamic-sized Nonblocking Hash table

hash表在进行resize的过程中保持Lock-Free是一件非常困难的事，因为需要将旧桶的数据读出，重新hash后放到新桶中。这个过程必须是原子的，看起来需要加锁来实现。Liu等人则提出一个新颖的概念：**一个bucket的数据是可以冻结的**。

```c++
class HandleTable {
public:
    HandleTable() : length_(0), elems_(0), list_(nullptr) {
        Resize();
    }

    ~HandleTable() {
        delete[] list_;
    }

    LRUHandle* Lookup(const Slice& key, uint32_t hash) {
        return *FindPointer(key, hash);
    }

    LRUHandle* Insert(LRUHandle* h) {
        LRUHandle** ptr = FindPointer(h->key(), h->hash);
        LRUHandle* old = *ptr;
        h->next_hash = (old == nullptr ? nullptr : old->next_hash);
        *ptr = h;
        if (old == nullptr) {
            ++elems_;
            if (elems_ > length_) {
                Resize();
            }
        }
        return old;
    }

    LRUHandle* Remove(const Slice& key, uint32_t hash) {
        LRUHandle** ptr = FindPointer(key, hash);
        LRUHandle* result = *ptr;
        if (result != nullptr) {
            *ptr = result->next_hash;
            --elems_;
        }
        return result;
    }

private:
    uint32_t length_;
    uint32_t elems_;
    // array of LRUHandle*
    LRUHandle** list_;

    // Return a pointer to slot that pointers to a cache entry that
    // matches key/hash. If there is no such cache entry, return a
    // pointer to the trailing slot in the corresponding linked list.
    LRUHandle** FindPointer(const Slice& key, uint32_t hash) {
        // find position by hash
        LRUHandle** ptr = &list_[hash & (length_ - 1)];
        while (*ptr != nullptr && ((*ptr)->hash != hash || key != (*ptr)->key())) {
            ptr = &(*ptr)->next_hash;
        }
        return ptr;
    }

    void Resize() {
        // start
        uint32_t new_length = 4;
        // expand
        while (new_length < elems_) {
            new_length *= 2;
        }
        LRUHandle** new_list = new LRUHandle*[new_length];
        memset(new_list, 0, sizeof(new_list[0]) * new_length);
        uint32_t count = 0;
        // reverse copy
        for (uint32_t i = 0; i < length_; ++i) {
            LRUHandle* h = list_[i];
            while (h != nullptr) {
                LRUHandle* next = h->next_hash;
                uint32_t hash = h->hash;
                // new position
                LRUHandle** ptr = &new_list[hash & (new_length - 1)];
                h->next_hash = *ptr;
                *ptr = h;
                h = next;
                ++count;
            }
        }
        assert(elems_ == count);
        delete[] list_;
        list_ = new_list;
        length_ = new_length;
    }
};
```

下面就可以实现最核心的数据结构：**LRU**。

LRU类的声明很简单，因为涉及到并发的问题，因此需要用锁来保护。具体的方法有外部的，有内部的，区分这两者可以使得实现更加清晰，同时最小化暴露。这里只是其中一个单一的分片。

LRUHandle有三种状态：

1. 对象的引用计数为0，清理
2. 对象的引用计数为1，但是没有外部持有，当容量满的时候，清理（存在于`lru_`）
3. 对象的引用计数超过1，不会被清理（存在于`in_use_`）

```c++
// A single shard of sharded cache
class LRUCache {
public:
    LRUCache();
    ~LRUCache();

    void SetCapacity(size_t capacity) {
        capacity_ = capacity;
    }

    Cache::Handle* Insert(const Slice& key, uint32_t hash, void* value,
                        size_t charge,
                        void (*deleter)(const Slice& key, void* value));
    Cache::Handle* Lookup(const Slice& key, uint32_t hash);
    void Release(Cache::Handle* handle);
    void Erase(const Slice& key, uint32_t hash);
    void Prune();
    size_t TotalCharge() const {
        MutexLock l(&mutex_);
        return usage_;
    }

private:
    void LRU_Remove(LRUHandle* e);
    void LRU_Append(LRUHandle* list, LRUHandle* e);
    void Ref(LRUHandle* e);
    void Unref(LRUHandle* e);
    bool FinishErase(LRUHandle* e) EXCLUSIVE_LOCKS_REQUIRED(mutex_);

    // Initialized before use
    size_t capacity_;

    // mutex_ protects the following stats
    mutable port::Mutex mutex_;
    size_t usage_ GUARDED_BY(mutex_);

    // Dummy head of LRU list
    LRUHandle lru_ GUARDED_BY(mutex_);

    // Dummy head of in-use list
    LRUHandle in_use_ GUARDED_BY(mutex_);

    HandleTable table_ GUARDED_BY(mutex_);
};
```

实现也比较直接。有两个链表，`lru_`和`in_use_`，其中`in_use_`是使用的比较频繁，而`lru_`则是快要淘汰的。这样的两层缓冲，可以提高缓存的效果。

```c++
LRUCache::LRUCache() : capacity_(0), usage_(0) {
    // Make empty circular linked lists
    lru_.next = &lru_;
    lru_.prev = &lru_;
    in_use_.next = &in_use_;
    in_use_.prev = &in_use_;
}

LRUCache::~LRUCache() {
    assert(in_use_.next == &in_use_);   // Error if caller has an unreleased handle
    for (LRUHandle* e = lru_.next; e != &lru_; ) {
        LRUHandle* next = e->next;
        assert(e->in_cache);
        e->in_cache = false;
        assert(e->refs == 1);
        Unref(e);
        e = next;
    }
}

// add reference
void LRUCache::Ref(LRUHandle* e) {
    if (e->refs == 1 && e->in_cache) {  // If on lru_ list, move to in_use_ list
        LRU_Remove(e);
        LRU_Append(&in_use_, e);
    }
    ++e->refs;
}

// remove reference
void LRUCache::Unref(LRUHandle* e) {
    assert(e->refs > 0);
    --e->refs;
    if (e->refs == 0) {     // Deallocate
        assert(!e->in_cache);
        (*e->deleter)(e->key(), e->value);
        free(e);
    } else if (e->in_cache && e->refs == 1) {   // Move to lru_
        LRU_Remove(e);
        LRU_Append(&lru_, e);
    }
}

void LRUCache::LRU_Remove(LRUHandle* e) {
    e->next->prev = e->prev;
    e->prev->next = e->next;
}

void LRUCache::LRU_Append(LRUHandle* list, LRUHandle* e) {
    // Make "e" newest entry by inserting just before *list
    e->next = list;
    e->prev = list->prev;
    e->prev->next = e;
    e->next->prev = e;
}

Cache::Handle* LRUCache::Lookup(const Slice& key, uint32_t hash) {
    MutexLock l(&mutex_);
    LRUHandle* e = table_.Lookup(key, hash);
    if (e != nullptr) {
        // Add reference
        Ref(e);
    }
    return reinterpret_cast<Cache::Handle*>(e);
}

void LRUCache::Release(Cache::Handle* handle) {
    MutexLock l(&mutex_);
    // Remove reference
    Unref(reinterpret_cast<LRUHandle*>(handle));
}

Cache::Handle* LRUCache::Insert(const Slice& key, uint32_t hash, void* value,
                                size_t charge,
                                void (*deleter)(const Slice& key,
                                                void* value)) {
    MutexLock l(&mutex_);

    LRUHandle* e = 
        reinterpret_cast<LRUHandle*>(malloc(sizeof(LRUHandle) - 1 + key.size()));
    e->value = value;
    e->deleter = deleter;
    e->charge = charge;
    e->key_length = key.size();
    e->hash = hash;
    e->in_cache = false;
    e->refs = 1;
    std::memcpy(e->key_data, key.data(), key.size());

    if (capacity_ > 0) {
        ++e->refs;
        e->in_cache = true;
        LRU_Append(&in_use_, e);
        usage_ += charge;
        FinishErase(table_.Insert(e));
    } else {
        // don't cache
        e->next = nullptr;
    }
  	// clear
    while (usage_ > capacity_ && lru_.next != &lru_) {
      	// remove from the oldest
        LRUHandle* old = lru_.next;
        assert(old->refs == 1);
        bool erased = FinishErase(table_.Remove(old->key(), old->hash));
        if (!erased) {
            assert(erased);
        }
    }

    return reinterpret_cast<Cache::Handle*>(e);
}

// If e != nullptr, finish removing *e from the cache; it has already been
// removed from the hash table. Return wether e != nullptr
bool LRUCache::FinishErase(LRUHandle* e) {
    if (e != nullptr) {
        assert(e->in_cache);
        LRU_Remove(e);
        e->in_cache = false;
        usage_ -= e->charge;
        Unref(e);
    }
    return e != nullptr;
}

void LRUCache::Erase(const Slice& key, uint32_t hash) {
    MutexLock l(&mutex_);
    FinishErase(table_.Remove(key, hash));
}

void LRUCache::Prune() {
    MutexLock l(&mutex_);
    while (lru_.next != &lru_) {
        LRUHandle* e = lru_.next;
        assert(e->refs == 1);
        bool erased = FinishErase(table_.Remove(e->key(), e->hash));
        if (!erased) {
            assert(erased);
        }
    }
}
```

本质就是使用了双向链表和Hash表。但是实现的过程比LeetCode上的那道题看上去复杂很多。`LRUCache`有大量锁的操作，为了减少锁的持有和更高的缓存命中率，就可以定义多个`LRUCache`。

另外就是实现一个多分片的LRU Cache，多分片的意义在于：提高并发度。根据key的哈希结果映射到不同的分片上，这样就。

```c++
static const int kNumShardBits = 4;
static const int kNumShards = 1 << kNumShardBits;

class ShardedLRUCache : public Cache {
public:
    explicit ShardedLRUCache(size_t capacity) : last_id_(0) {
        const size_t per_shard = (capacity + (kNumShards - 1)) / kNumShards;
        for (int s = 0; s < kNumShards; ++s) {
            shard_[s].SetCapacity(per_shard);
        }
    }
    ~ShardedLRUCache() override {}
    Handle* Insert(const Slice& key, void* value, size_t charge,
                    void (*deleter)(const Slice& key, void* value)) override {
        const uint32_t hash = HashSlice(key);
        return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter);
    }
    Handle* Lookup(const Slice& key) override {
        const uint32_t hash = HashSlice(key);
        return shard_[Shard(hash)].Lookup(key, hash);
    }
    void Release(Handle* handle) override {
        LRUHandle* h = reinterpret_cast<LRUHandle*>(handle);
        shard_[Shard(h->hash)].Release(handle);
    }
    void Erase(const Slice& key) override {
        const uint32_t hash = HashSlice(key);
        shard_[Shard(hash)].Erase(key, hash);
    }
    void* Value(Handle* handle) override {
        return reinterpret_cast<LRUHandle*>(handle)->value;
    }
    uint64_t NewId() override {
        MutexLock l(&id_mutex_);
        return ++last_id_;
    }
    void Prune() override {
        for (int s = 0; s < kNumShards; ++s) {
            shard_[s].Prune();
        }
    }
    size_t TotalCharge() const override {
        size_t total = 0;
        for (int s = 0; s < kNumShardBits; ++s) {
            total += shard_[s].TotalCharge();
        }
        return total;
    }
private:
    LRUCache shard_[kNumShards];
    port::Mutex id_mutex_;
    uint64_t last_id_;

    static inline uint32_t HashSlice(const Slice& s) {
        return Hash(s.data(), s.size(), 0);
    }

    static uint32_t Shard(uint32_t hash) {
        return hash >> (32 - kNumShardBits);
    }
};
```



#### 理解



#### 参考

- https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.711.8782&rep=rep1&type=pdf
- https://leveldb-handbook.readthedocs.io/zh/latest/cache.html
- https://izualzhy.cn/leveldb-cache